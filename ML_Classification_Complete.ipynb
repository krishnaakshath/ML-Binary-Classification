{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83e\udde0 Comprehensive ML Classification Project\n",
    "## Binary & Multi-Class Classification Pipeline\n",
    "---\n",
    "**Datasets:**\n",
    "- Binary: Breast Cancer, Diabetes (Pima), Titanic\n",
    "- Multi-Class: Iris, Wine Quality, Dry Bean\n",
    "\n",
    "**Models:** Logistic Regression, Decision Tree, Random Forest, SVM, KNN, XGBoost\n",
    "\n",
    "**Evaluation:** Accuracy, Precision, Recall, F1, ROC-AUC, Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Part 1: Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (for Colab)\n",
    "!pip install -q xgboost openpyxl imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV,\n",
    "                                     cross_val_score, learning_curve, StratifiedKFold)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, label_binarize\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix,\n",
    "                             classification_report, roc_curve, auc)\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"\u2705 All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce5 Part 2: Data Loading\n",
    "**Upload all dataset files to Colab** using the file upload cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "print(\"\ud83d\udcc2 Upload ALL dataset files:\")\n",
    "print(\"  1. data.csv (Breast Cancer)\")\n",
    "print(\"  2. diabetes.csv (Pima Diabetes)\")\n",
    "print(\"  3. train.csv (Titanic)\")\n",
    "print(\"  4. Iris.csv\")\n",
    "print(\"  5. winequality-red.csv\")\n",
    "print(\"  6. Dry_Bean_Dataset.xlsx\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets\n",
    "df_cancer = pd.read_csv('data.csv')\n",
    "df_diabetes = pd.read_csv('diabetes.csv')\n",
    "df_titanic = pd.read_csv('train.csv')\n",
    "df_iris = pd.read_csv('Iris.csv')\n",
    "df_wine = pd.read_csv('winequality-red.csv')\n",
    "df_beans = pd.read_excel('Dry_Bean_Dataset.xlsx')\n",
    "\n",
    "datasets = {\n",
    "    'Breast Cancer': df_cancer, 'Diabetes': df_diabetes,\n",
    "    'Titanic': df_titanic, 'Iris': df_iris,\n",
    "    'Wine Quality': df_wine, 'Dry Bean': df_beans\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"\ud83d\udcca {name}: {df.shape[0]} rows \u00d7 {df.shape[1]} columns\")\n",
    "    print(f\"   Missing: {df.isnull().sum().sum()} | Duplicates: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# \ud83d\udd0d Part 3: Exploratory Data Analysis (EDA)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 \u2014 Breast Cancer Dataset EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcca BREAST CANCER DATASET\")\n",
    "print(f\"Shape: {df_cancer.shape}\")\n",
    "print(f\"\\nTarget Distribution:\\n{df_cancer['diagnosis'].value_counts()}\")\n",
    "print(f\"\\nData Types:\\n{df_cancer.dtypes.value_counts()}\")\n",
    "df_cancer.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "sns.countplot(data=df_cancer, x='diagnosis', ax=axes[0], palette='coolwarm')\n",
    "axes[0].set_title('Diagnosis Distribution (M=Malignant, B=Benign)', fontsize=12)\n",
    "for p in axes[0].patches:\n",
    "    axes[0].annotate(f'{int(p.get_height())}', (p.get_x()+p.get_width()/2., p.get_height()),\n",
    "                     ha='center', va='bottom', fontsize=11)\n",
    "sns.histplot(data=df_cancer, x='radius_mean', hue='diagnosis', kde=True, ax=axes[1], palette='coolwarm')\n",
    "axes[1].set_title('Radius Mean by Diagnosis')\n",
    "sns.histplot(data=df_cancer, x='concavity_mean', hue='diagnosis', kde=True, ax=axes[2], palette='coolwarm')\n",
    "axes[2].set_title('Concavity Mean by Diagnosis')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cols = [c for c in df_cancer.columns if '_mean' in c]\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.heatmap(df_cancer[mean_cols].corr(), annot=True, fmt='.2f', cmap='RdBu_r', center=0, ax=ax)\n",
    "ax.set_title('Correlation Heatmap \u2014 Mean Features (Breast Cancer)', fontsize=14)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 \u2014 Diabetes Dataset EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcca DIABETES DATASET\")\n",
    "print(f\"Shape: {df_diabetes.shape}\")\n",
    "print(f\"\\nTarget Distribution:\\n{df_diabetes['Outcome'].value_counts()}\")\n",
    "zero_cols = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\n",
    "print(f\"\\nZero values in biological features (likely missing):\")\n",
    "for col in zero_cols:\n",
    "    print(f\"  {col}: {(df_diabetes[col]==0).sum()} zeros ({(df_diabetes[col]==0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "for i, col in enumerate(df_diabetes.columns[:-1]):\n",
    "    ax = axes[i//4, i%4]\n",
    "    sns.histplot(data=df_diabetes, x=col, hue='Outcome', kde=True, ax=ax, palette='Set1')\n",
    "    ax.set_title(col, fontsize=11)\n",
    "plt.suptitle('Diabetes Dataset \u2014 Feature Distributions by Outcome', fontsize=14, y=1.02)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "sns.heatmap(df_diabetes.corr(), annot=True, fmt='.2f', cmap='YlOrRd', ax=ax)\n",
    "ax.set_title('Correlation Heatmap \u2014 Diabetes', fontsize=14)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 \u2014 Titanic Dataset EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcca TITANIC DATASET\")\n",
    "print(f\"Shape: {df_titanic.shape}\")\n",
    "print(f\"\\nTarget Distribution:\\n{df_titanic['Survived'].value_counts()}\")\n",
    "print(f\"\\nMissing Values:\\n{df_titanic.isnull().sum()[df_titanic.isnull().sum()>0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "sns.countplot(data=df_titanic, x='Survived', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Survival Distribution')\n",
    "sns.countplot(data=df_titanic, x='Pclass', hue='Survived', ax=axes[1], palette='Set2')\n",
    "axes[1].set_title('Survival by Class')\n",
    "sns.countplot(data=df_titanic, x='Sex', hue='Survived', ax=axes[2], palette='Set2')\n",
    "axes[2].set_title('Survival by Sex')\n",
    "sns.histplot(data=df_titanic, x='Age', hue='Survived', kde=True, ax=axes[3], palette='Set2')\n",
    "axes[3].set_title('Survival by Age')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "sns.boxplot(data=df_titanic, x='Pclass', y='Fare', hue='Survived', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Fare by Class & Survival')\n",
    "sns.countplot(data=df_titanic, x='Embarked', hue='Survived', ax=axes[1], palette='Set2')\n",
    "axes[1].set_title('Survival by Embarkation Port')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 \u2014 Iris Dataset EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcca IRIS DATASET\")\n",
    "print(f\"Shape: {df_iris.shape}\")\n",
    "print(f\"\\nTarget Distribution:\\n{df_iris['Species'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "for i, col in enumerate(['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']):\n",
    "    ax = axes[i//2, i%2]\n",
    "    sns.boxplot(data=df_iris, x='Species', y=col, ax=ax, palette='viridis')\n",
    "    ax.set_title(f'{col} by Species', fontsize=12)\n",
    "plt.suptitle('Iris Dataset \u2014 Feature Distributions', fontsize=14, y=1.02)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_iris, hue='Species', vars=['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm'],\n",
    "             palette='viridis', diag_kind='kde')\n",
    "plt.suptitle('Iris Pairplot', y=1.02, fontsize=14); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 \u2014 Wine Quality Dataset EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcca WINE QUALITY DATASET\")\n",
    "print(f\"Shape: {df_wine.shape}\")\n",
    "print(f\"\\nQuality Distribution:\\n{df_wine['quality'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "for i, col in enumerate(['fixed acidity','volatile acidity','citric acid','alcohol','sulphates','pH']):\n",
    "    ax = axes[i//3, i%3]\n",
    "    sns.boxplot(data=df_wine, x='quality', y=col, ax=ax, palette='magma')\n",
    "    ax.set_title(f'{col} by Quality')\n",
    "plt.suptitle('Wine Quality \u2014 Key Features', fontsize=14, y=1.02)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.heatmap(df_wine.corr(), annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax)\n",
    "ax.set_title('Correlation Heatmap \u2014 Wine Quality', fontsize=14)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 \u2014 Dry Bean Dataset EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcca DRY BEAN DATASET\")\n",
    "print(f\"Shape: {df_beans.shape}\")\n",
    "print(f\"\\nClass Distribution:\\n{df_beans['Class'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(22, 10))\n",
    "num_cols = df_beans.select_dtypes(include=np.number).columns[:8]\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = axes[i//4, i%4]\n",
    "    sns.boxplot(data=df_beans, x='Class', y=col, ax=ax, palette='Set3')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.set_title(col, fontsize=11)\n",
    "plt.suptitle('Dry Bean \u2014 Feature Distributions by Class', fontsize=14, y=1.02)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# \ud83e\uddf9 Part 4: Data Cleaning & Preprocessing\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 \u2014 Outlier Detection & Handling (IQR Method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, columns):\n",
    "    \"\"\"Detect outliers using IQR method and return summary.\"\"\"\n",
    "    outlier_info = {}\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
    "        outlier_info[col] = {'count': len(outliers), 'pct': len(outliers)/len(df)*100,\n",
    "                             'lower': lower, 'upper': upper}\n",
    "    return outlier_info\n",
    "\n",
    "def cap_outliers(df, columns):\n",
    "    \"\"\"Cap outliers at IQR boundaries (Winsorization).\"\"\"\n",
    "    df_capped = df.copy()\n",
    "    for col in columns:\n",
    "        Q1 = df_capped[col].quantile(0.25)\n",
    "        Q3 = df_capped[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        df_capped[col] = df_capped[col].clip(lower, upper)\n",
    "    return df_capped\n",
    "\n",
    "# Show outlier detection for each dataset\n",
    "for name, df in [('Breast Cancer', df_cancer), ('Diabetes', df_diabetes), ('Wine Quality', df_wine)]:\n",
    "    num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    info = detect_outliers_iqr(df, num_cols)\n",
    "    total_outliers = sum(v['count'] for v in info.values())\n",
    "    top_outliers = sorted(info.items(), key=lambda x: x[1]['count'], reverse=True)[:3]\n",
    "    print(f\"\\n\ud83d\udcca {name} \u2014 Total outlier instances: {total_outliers}\")\n",
    "    for col, v in top_outliers:\n",
    "        print(f\"   {col}: {v['count']} outliers ({v['pct']:.1f}%)\")\n",
    "\n",
    "print(\"\\n\u2705 Outlier detection complete! Using IQR capping (Winsorization) during preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 \u2014 Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ BREAST CANCER ============\n",
    "df_cancer_clean = df_cancer.drop(['id'], axis=1)\n",
    "df_cancer_clean = df_cancer_clean.loc[:, ~df_cancer_clean.columns.str.contains('^Unnamed')]\n",
    "df_cancer_clean['diagnosis'] = df_cancer_clean['diagnosis'].map({'M': 1, 'B': 0})\n",
    "# Cap outliers\n",
    "num_cols_cancer = df_cancer_clean.select_dtypes(include=np.number).columns.drop('diagnosis').tolist()\n",
    "df_cancer_clean = cap_outliers(df_cancer_clean, num_cols_cancer)\n",
    "print(f\"\u2705 Breast Cancer cleaned + outliers capped: {df_cancer_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ DIABETES ============\n",
    "df_diabetes_clean = df_diabetes.copy()\n",
    "zero_cols = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\n",
    "df_diabetes_clean[zero_cols] = df_diabetes_clean[zero_cols].replace(0, np.nan)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_diabetes_clean[zero_cols] = imputer.fit_transform(df_diabetes_clean[zero_cols])\n",
    "# Cap outliers\n",
    "num_cols_diab = df_diabetes_clean.columns.drop('Outcome').tolist()\n",
    "df_diabetes_clean = cap_outliers(df_diabetes_clean, num_cols_diab)\n",
    "print(f\"\u2705 Diabetes cleaned + outliers capped: {df_diabetes_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TITANIC ============\n",
    "df_titanic_clean = df_titanic.copy()\n",
    "df_titanic_clean['Age'].fillna(df_titanic_clean['Age'].median(), inplace=True)\n",
    "df_titanic_clean['Embarked'].fillna(df_titanic_clean['Embarked'].mode()[0], inplace=True)\n",
    "df_titanic_clean['FamilySize'] = df_titanic_clean['SibSp'] + df_titanic_clean['Parch'] + 1\n",
    "df_titanic_clean['IsAlone'] = (df_titanic_clean['FamilySize'] == 1).astype(int)\n",
    "df_titanic_clean['Title'] = df_titanic_clean['Name'].str.extract(r' ([A-Za-z]+)\\.')\n",
    "df_titanic_clean['Title'] = df_titanic_clean['Title'].replace(\n",
    "    ['Lady','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'], 'Rare')\n",
    "df_titanic_clean['Title'] = df_titanic_clean['Title'].replace(['Mlle','Ms'], 'Miss')\n",
    "df_titanic_clean['Title'] = df_titanic_clean['Title'].replace('Mme', 'Mrs')\n",
    "df_titanic_clean['Sex'] = df_titanic_clean['Sex'].map({'male': 0, 'female': 1})\n",
    "df_titanic_clean = pd.get_dummies(df_titanic_clean, columns=['Embarked', 'Title'], drop_first=True)\n",
    "df_titanic_clean.drop(['PassengerId','Name','Ticket','Cabin'], axis=1, inplace=True)\n",
    "print(f\"\u2705 Titanic cleaned: {df_titanic_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ IRIS ============\n",
    "df_iris_clean = df_iris.drop('Id', axis=1)\n",
    "le_iris = LabelEncoder()\n",
    "df_iris_clean['Species_encoded'] = le_iris.fit_transform(df_iris_clean['Species'])\n",
    "print(f\"\u2705 Iris cleaned: {df_iris_clean.shape}\")\n",
    "print(f\"   Classes: {dict(zip(le_iris.classes_, le_iris.transform(le_iris.classes_)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ WINE QUALITY ============\n",
    "df_wine_clean = df_wine.copy()\n",
    "num_cols_wine = df_wine_clean.columns.drop('quality').tolist()\n",
    "df_wine_clean = cap_outliers(df_wine_clean, num_cols_wine)\n",
    "print(f\"\u2705 Wine Quality cleaned + outliers capped: {df_wine_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ DRY BEAN ============\n",
    "df_beans_clean = df_beans.copy()\n",
    "df_beans_clean.dropna(inplace=True)\n",
    "le_beans = LabelEncoder()\n",
    "df_beans_clean['Class_encoded'] = le_beans.fit_transform(df_beans_clean['Class'])\n",
    "num_cols_beans = df_beans_clean.select_dtypes(include=np.number).columns.drop('Class_encoded').tolist()\n",
    "df_beans_clean = cap_outliers(df_beans_clean, num_cols_beans)\n",
    "print(f\"\u2705 Dry Bean cleaned + outliers capped: {df_beans_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# \ud83c\udfaf Part 5: Feature Selection (SelectKBest)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_analysis(X, y, feature_names, dataset_name, k='all'):\n",
    "    \"\"\"Perform feature selection using ANOVA F-test and Mutual Information.\"\"\"\n",
    "    # ANOVA F-test\n",
    "    selector_f = SelectKBest(score_func=f_classif, k=k)\n",
    "    selector_f.fit(X, y)\n",
    "    f_scores = pd.DataFrame({'Feature': feature_names, 'F-Score': selector_f.scores_,\n",
    "                             'p-value': selector_f.pvalues_}).sort_values('F-Score', ascending=False)\n",
    "\n",
    "    # Mutual Information\n",
    "    mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "    mi_df = pd.DataFrame({'Feature': feature_names, 'MI-Score': mi_scores}).sort_values('MI-Score', ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    top_n = min(15, len(feature_names))\n",
    "    sns.barplot(data=f_scores.head(top_n), x='F-Score', y='Feature', ax=axes[0], palette='viridis')\n",
    "    axes[0].set_title(f'{dataset_name} \u2014 Top Features (ANOVA F-Score)')\n",
    "    sns.barplot(data=mi_df.head(top_n), x='MI-Score', y='Feature', ax=axes[1], palette='magma')\n",
    "    axes[1].set_title(f'{dataset_name} \u2014 Top Features (Mutual Information)')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Print top features\n",
    "    print(f\"\\n\ud83c\udfc6 {dataset_name} \u2014 Top 5 Features:\")\n",
    "    for i, row in f_scores.head(5).iterrows():\n",
    "        print(f\"   {row['Feature']}: F={row['F-Score']:.2f}, p={row['p-value']:.2e}\")\n",
    "    return f_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast Cancer\n",
    "X_cancer = df_cancer_clean.drop('diagnosis', axis=1)\n",
    "fs_cancer = feature_selection_analysis(X_cancer.values, df_cancer_clean['diagnosis'],\n",
    "                                       X_cancer.columns.tolist(), 'Breast Cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes\n",
    "X_diab = df_diabetes_clean.drop('Outcome', axis=1)\n",
    "fs_diabetes = feature_selection_analysis(X_diab.values, df_diabetes_clean['Outcome'],\n",
    "                                          X_diab.columns.tolist(), 'Diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris\n",
    "X_iris_fs = df_iris_clean[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n",
    "fs_iris = feature_selection_analysis(X_iris_fs.values, df_iris_clean['Species_encoded'],\n",
    "                                      X_iris_fs.columns.tolist(), 'Iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine Quality\n",
    "X_wine_fs = df_wine_clean.drop('quality', axis=1)\n",
    "fs_wine = feature_selection_analysis(X_wine_fs.values, df_wine_clean['quality'],\n",
    "                                      X_wine_fs.columns.tolist(), 'Wine Quality')\n",
    "\n",
    "print(\"\\n\u2705 Feature selection analysis complete for all datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# \ud83e\udd16 Part 6: Enhanced Model Training & Evaluation Pipeline\n",
    "---\n",
    "Includes: Cross-Validation, SMOTE, Classification Reports, Feature Importance, Learning Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    \"\"\"Return dictionary of all models to train.\"\"\"\n",
    "    return {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=5000, random_state=42),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'SVM': SVC(probability=True, random_state=42),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42, use_label_encoder=False)\n",
    "    }\n",
    "\n",
    "def train_evaluate(X_train, X_test, y_train, y_test, dataset_name, task='binary', use_smote=False):\n",
    "    \"\"\"Train all models with optional SMOTE, cross-validation & return results.\"\"\"\n",
    "    models = get_models()\n",
    "    results = []\n",
    "    trained = {}\n",
    "    cv_results = {}\n",
    "\n",
    "    # Apply SMOTE if requested (for imbalanced datasets)\n",
    "    if use_smote:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_fit, y_train_fit = smote.fit_resample(X_train, y_train)\n",
    "        print(f\"  \ud83d\udcca SMOTE applied: {len(y_train)} \u2192 {len(y_train_fit)} samples\")\n",
    "    else:\n",
    "        X_train_fit, y_train_fit = X_train, y_train\n",
    "\n",
    "    for name, model in models.items():\n",
    "        # Cross-validation (5-fold)\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scoring = 'f1' if task == 'binary' else 'f1_weighted'\n",
    "        cv_scores = cross_val_score(model, X_train_fit, y_train_fit, cv=cv, scoring=scoring)\n",
    "        cv_results[name] = cv_scores\n",
    "\n",
    "        # Train on full training set\n",
    "        model.fit(X_train_fit, y_train_fit)\n",
    "        y_pred = model.predict(X_test)\n",
    "        trained[name] = model\n",
    "\n",
    "        avg = 'binary' if task == 'binary' else 'weighted'\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average=avg, zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average=avg, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average=avg, zero_division=0)\n",
    "\n",
    "        try:\n",
    "            if task == 'binary':\n",
    "                y_prob = model.predict_proba(X_test)[:, 1]\n",
    "                auc_score = roc_auc_score(y_test, y_prob)\n",
    "            else:\n",
    "                y_prob = model.predict_proba(X_test)\n",
    "                n_classes = len(np.unique(y_test))\n",
    "                y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))\n",
    "                if y_test_bin.shape[1] == 1:\n",
    "                    y_test_bin = np.hstack([1 - y_test_bin, y_test_bin])\n",
    "                auc_score = roc_auc_score(y_test_bin, y_prob, multi_class='ovr', average='weighted')\n",
    "        except:\n",
    "            auc_score = np.nan\n",
    "\n",
    "        results.append({\n",
    "            'Model': name, 'Accuracy': acc, 'Precision': prec,\n",
    "            'Recall': rec, 'F1-Score': f1, 'ROC-AUC': auc_score,\n",
    "            'CV-Mean': cv_scores.mean(), 'CV-Std': cv_scores.std()\n",
    "        })\n",
    "        print(f\"  \u2705 {name}: Acc={acc:.4f}, F1={f1:.4f}, AUC={auc_score:.4f}, CV={cv_scores.mean():.4f}\u00b1{cv_scores.std():.4f}\")\n",
    "\n",
    "    return pd.DataFrame(results), trained, cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_df, dataset_name):\n",
    "    \"\"\"Plot model comparison charts.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    results_melted = results_df[['Model','Accuracy','Precision','Recall','F1-Score','ROC-AUC']].melt(\n",
    "        id_vars='Model', var_name='Metric', value_name='Score')\n",
    "    sns.barplot(data=results_melted, x='Model', y='Score', hue='Metric', ax=axes[0], palette='viridis')\n",
    "    axes[0].set_title(f'{dataset_name} \u2014 Model Comparison', fontsize=13)\n",
    "    axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=30, ha='right')\n",
    "    axes[0].set_ylim(0, 1.05)\n",
    "    axes[0].legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "    heatmap_data = results_df.set_index('Model')[['Accuracy','Precision','Recall','F1-Score','ROC-AUC']]\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlGn', ax=axes[1], vmin=0, vmax=1)\n",
    "    axes[1].set_title(f'{dataset_name} \u2014 Scores Heatmap', fontsize=13)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_cv_comparison(cv_results, dataset_name):\n",
    "    \"\"\"Plot cross-validation score distributions.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    cv_data = pd.DataFrame(cv_results)\n",
    "    ax.boxplot(cv_data.values, labels=cv_data.columns)\n",
    "    ax.set_title(f'{dataset_name} \u2014 5-Fold Cross-Validation Scores', fontsize=13)\n",
    "    ax.set_ylabel('F1-Score')\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_confusion_matrices(X_test, y_test, trained_models, dataset_name):\n",
    "    \"\"\"Plot confusion matrices for all models.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    for idx, (name, model) in enumerate(trained_models.items()):\n",
    "        ax = axes[idx//3, idx%3]\n",
    "        y_pred = model.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "        ax.set_title(f'{name}', fontsize=11)\n",
    "        ax.set_ylabel('Actual'); ax.set_xlabel('Predicted')\n",
    "    plt.suptitle(f'{dataset_name} \u2014 Confusion Matrices', fontsize=14, y=1.02)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_roc_curves(X_test, y_test, trained_models, dataset_name):\n",
    "    \"\"\"Plot ROC curves for binary classification.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    for name, model in trained_models.items():\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax.plot(fpr, tpr, label=f'{name} (AUC={roc_auc:.3f})')\n",
    "    ax.plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "    ax.set_xlabel('False Positive Rate'); ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f'{dataset_name} \u2014 ROC Curves', fontsize=14)\n",
    "    ax.legend(loc='lower right'); plt.tight_layout(); plt.show()\n",
    "\n",
    "def print_classification_reports(X_test, y_test, trained_models, dataset_name, target_names=None):\n",
    "    \"\"\"Print detailed classification reports for all models.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"\ud83d\udccb CLASSIFICATION REPORTS \u2014 {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for name, model in trained_models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"\\n\ud83d\udd39 {name}:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))\n",
    "\n",
    "def plot_feature_importance(trained_models, feature_names, dataset_name):\n",
    "    \"\"\"Plot feature importance for tree-based models.\"\"\"\n",
    "    tree_models = {n: m for n, m in trained_models.items()\n",
    "                   if hasattr(m, 'feature_importances_')}\n",
    "    if not tree_models:\n",
    "        return\n",
    "    n = len(tree_models)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(7*n, 6))\n",
    "    if n == 1: axes = [axes]\n",
    "    for idx, (name, model) in enumerate(tree_models.items()):\n",
    "        imp = pd.DataFrame({'Feature': feature_names, 'Importance': model.feature_importances_})\n",
    "        imp = imp.sort_values('Importance', ascending=True).tail(15)\n",
    "        axes[idx].barh(imp['Feature'], imp['Importance'], color=plt.cm.viridis(np.linspace(0.3, 0.9, len(imp))))\n",
    "        axes[idx].set_title(f'{name} \u2014 Feature Importance', fontsize=12)\n",
    "        axes[idx].set_xlabel('Importance')\n",
    "    plt.suptitle(f'{dataset_name} \u2014 Feature Importance (Top 15)', fontsize=14, y=1.02)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_learning_curves(model, model_name, X_train, y_train, dataset_name, task='binary'):\n",
    "    \"\"\"Plot learning curves to check overfitting/underfitting.\"\"\"\n",
    "    scoring = 'f1' if task == 'binary' else 'f1_weighted'\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X_train, y_train, cv=5, scoring=scoring,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10), random_state=42, n_jobs=-1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(train_sizes, train_scores.mean(axis=1), 'o-', label='Training Score', color='#2196F3')\n",
    "    ax.fill_between(train_sizes, train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                    train_scores.mean(axis=1) + train_scores.std(axis=1), alpha=0.15, color='#2196F3')\n",
    "    ax.plot(train_sizes, val_scores.mean(axis=1), 'o-', label='Validation Score', color='#E91E63')\n",
    "    ax.fill_between(train_sizes, val_scores.mean(axis=1) - val_scores.std(axis=1),\n",
    "                    val_scores.mean(axis=1) + val_scores.std(axis=1), alpha=0.15, color='#E91E63')\n",
    "    ax.set_xlabel('Training Set Size'); ax.set_ylabel('F1-Score')\n",
    "    ax.set_title(f'{dataset_name} \u2014 Learning Curve ({model_name})', fontsize=13)\n",
    "    ax.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"\u2705 Enhanced pipeline functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# \ud83c\udfaf Part 7: Binary Classification \u2014 Training & Evaluation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 \u2014 Breast Cancer Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udd2c BREAST CANCER \u2014 Binary Classification\")\n",
    "print(\"=\"*60)\n",
    "X = df_cancer_clean.drop('diagnosis', axis=1)\n",
    "y = df_cancer_clean['diagnosis']\n",
    "feature_names_cancer = X.columns.tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "results_cancer, models_cancer, cv_cancer = train_evaluate(\n",
    "    X_train_sc, X_test_sc, y_train, y_test, 'Breast Cancer', 'binary')\n",
    "print(f\"\\n{results_cancer.to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_cancer, 'Breast Cancer')\n",
    "plot_cv_comparison(cv_cancer, 'Breast Cancer')\n",
    "plot_confusion_matrices(X_test_sc, y_test, models_cancer, 'Breast Cancer')\n",
    "plot_roc_curves(X_test_sc, y_test, models_cancer, 'Breast Cancer')\n",
    "print_classification_reports(X_test_sc, y_test, models_cancer, 'Breast Cancer', ['Benign', 'Malignant'])\n",
    "plot_feature_importance(models_cancer, feature_names_cancer, 'Breast Cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curve for best model\n",
    "best_name = results_cancer.loc[results_cancer['F1-Score'].idxmax(), 'Model']\n",
    "plot_learning_curves(get_models()[best_name], best_name, X_train_sc, y_train, 'Breast Cancer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 \u2014 Diabetes Classification (with SMOTE for class imbalance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83e\ude7a DIABETES \u2014 Binary Classification (with SMOTE)\")\n",
    "print(\"=\"*60)\n",
    "X = df_diabetes_clean.drop('Outcome', axis=1)\n",
    "y = df_diabetes_clean['Outcome']\n",
    "feature_names_diab = X.columns.tolist()\n",
    "print(f\"Class imbalance: {dict(y.value_counts())} \u2014 applying SMOTE\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "results_diabetes, models_diabetes, cv_diabetes = train_evaluate(\n",
    "    X_train_sc, X_test_sc, y_train, y_test, 'Diabetes', 'binary', use_smote=True)\n",
    "print(f\"\\n{results_diabetes.to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_diabetes, 'Diabetes')\n",
    "plot_cv_comparison(cv_diabetes, 'Diabetes')\n",
    "plot_confusion_matrices(X_test_sc, y_test, models_diabetes, 'Diabetes')\n",
    "plot_roc_curves(X_test_sc, y_test, models_diabetes, 'Diabetes')\n",
    "print_classification_reports(X_test_sc, y_test, models_diabetes, 'Diabetes', ['No Diabetes', 'Diabetes'])\n",
    "plot_feature_importance(models_diabetes, feature_names_diab, 'Diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = results_diabetes.loc[results_diabetes['F1-Score'].idxmax(), 'Model']\n",
    "plot_learning_curves(get_models()[best_name], best_name, X_train_sc, y_train, 'Diabetes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 \u2014 Titanic Survival Prediction (with SMOTE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udea2 TITANIC \u2014 Binary Classification (with SMOTE)\")\n",
    "print(\"=\"*60)\n",
    "X = df_titanic_clean.drop('Survived', axis=1)\n",
    "y = df_titanic_clean['Survived']\n",
    "feature_names_titanic = X.columns.tolist()\n",
    "print(f\"Class imbalance: {dict(y.value_counts())} \u2014 applying SMOTE\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "results_titanic, models_titanic, cv_titanic = train_evaluate(\n",
    "    X_train_sc, X_test_sc, y_train, y_test, 'Titanic', 'binary', use_smote=True)\n",
    "print(f\"\\n{results_titanic.to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_titanic, 'Titanic')\n",
    "plot_cv_comparison(cv_titanic, 'Titanic')\n",
    "plot_confusion_matrices(X_test_sc, y_test, models_titanic, 'Titanic')\n",
    "plot_roc_curves(X_test_sc, y_test, models_titanic, 'Titanic')\n",
    "print_classification_reports(X_test_sc, y_test, models_titanic, 'Titanic', ['Died', 'Survived'])\n",
    "plot_feature_importance(models_titanic, feature_names_titanic, 'Titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = results_titanic.loc[results_titanic['F1-Score'].idxmax(), 'Model']\n",
    "plot_learning_curves(get_models()[best_name], best_name, X_train_sc, y_train, 'Titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# \ud83c\udf08 Part 8: Multi-Class Classification \u2014 Training & Evaluation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 \u2014 Iris Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83c\udf38 IRIS \u2014 Multi-Class Classification\")\n",
    "print(\"=\"*60)\n",
    "X = df_iris_clean[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n",
    "y = df_iris_clean['Species_encoded']\n",
    "feature_names_iris = X.columns.tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "results_iris, models_iris, cv_iris = train_evaluate(\n",
    "    X_train_sc, X_test_sc, y_train, y_test, 'Iris', 'multi')\n",
    "print(f\"\\n{results_iris.to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_iris, 'Iris')\n",
    "plot_cv_comparison(cv_iris, 'Iris')\n",
    "plot_confusion_matrices(X_test_sc, y_test, models_iris, 'Iris')\n",
    "print_classification_reports(X_test_sc, y_test, models_iris, 'Iris', le_iris.classes_.tolist())\n",
    "plot_feature_importance(models_iris, feature_names_iris, 'Iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = results_iris.loc[results_iris['F1-Score'].idxmax(), 'Model']\n",
    "plot_learning_curves(get_models()[best_name], best_name, X_train_sc, y_train, 'Iris', 'multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 \u2014 Wine Quality Classification (with SMOTE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83c\udf77 WINE QUALITY \u2014 Multi-Class Classification (with SMOTE)\")\n",
    "print(\"=\"*60)\n",
    "X = df_wine_clean.drop('quality', axis=1)\n",
    "y = df_wine_clean['quality']\n",
    "le_wine = LabelEncoder()\n",
    "y = le_wine.fit_transform(y)\n",
    "feature_names_wine = X.columns.tolist()\n",
    "print(f\"Class distribution: {dict(zip(*np.unique(y, return_counts=True)))} \u2014 applying SMOTE\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "results_wine, models_wine, cv_wine = train_evaluate(\n",
    "    X_train_sc, X_test_sc, y_train, y_test, 'Wine Quality', 'multi', use_smote=True)\n",
    "print(f\"\\n{results_wine.to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_wine, 'Wine Quality')\n",
    "plot_cv_comparison(cv_wine, 'Wine Quality')\n",
    "plot_confusion_matrices(X_test_sc, y_test, models_wine, 'Wine Quality')\n",
    "print_classification_reports(X_test_sc, y_test, models_wine, 'Wine Quality')\n",
    "plot_feature_importance(models_wine, feature_names_wine, 'Wine Quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 \u2014 Dry Bean Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83e\uded8 DRY BEAN \u2014 Multi-Class Classification\")\n",
    "print(\"=\"*60)\n",
    "X = df_beans_clean.drop(['Class', 'Class_encoded'], axis=1)\n",
    "y = df_beans_clean['Class_encoded']\n",
    "feature_names_beans = X.columns.tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "results_beans, models_beans, cv_beans = train_evaluate(\n",
    "    X_train_sc, X_test_sc, y_train, y_test, 'Dry Bean', 'multi')\n",
    "print(f\"\\n{results_beans.to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_beans, 'Dry Bean')\n",
    "plot_cv_comparison(cv_beans, 'Dry Bean')\n",
    "plot_confusion_matrices(X_test_sc, y_test, models_beans, 'Dry Bean')\n",
    "print_classification_reports(X_test_sc, y_test, models_beans, 'Dry Bean', le_beans.classes_.tolist())\n",
    "plot_feature_importance(models_beans, feature_names_beans, 'Dry Bean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# \u26a1 Part 9: Hyperparameter Tuning (GridSearchCV)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "def tune_best_model(X_train, X_test, y_train, y_test, results_df, dataset_name, task='binary'):\n",
    "    \"\"\"Find best model and tune it with GridSearchCV.\"\"\"\n",
    "    best_model_name = results_df.loc[results_df['F1-Score'].idxmax(), 'Model']\n",
    "    print(f\"\\n\ud83c\udfc6 Best model for {dataset_name}: {best_model_name}\")\n",
    "\n",
    "    if best_model_name in param_grids:\n",
    "        tune_name = best_model_name\n",
    "    else:\n",
    "        tune_name = 'Random Forest'\n",
    "        print(f\"   \u2192 Tuning Random Forest instead (as proxy)\")\n",
    "\n",
    "    if tune_name == 'Random Forest':\n",
    "        base_model = RandomForestClassifier(random_state=42)\n",
    "    elif tune_name == 'XGBoost':\n",
    "        base_model = XGBClassifier(eval_metric='logloss', random_state=42, use_label_encoder=False)\n",
    "    else:\n",
    "        base_model = SVC(probability=True, random_state=42)\n",
    "\n",
    "    scoring = 'f1' if task == 'binary' else 'f1_weighted'\n",
    "    grid = GridSearchCV(base_model, param_grids[tune_name], cv=5, scoring=scoring, n_jobs=-1, verbose=0)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"   Best params: {grid.best_params_}\")\n",
    "    print(f\"   Best CV Score: {grid.best_score_:.4f}\")\n",
    "\n",
    "    y_pred = grid.best_estimator_.predict(X_test)\n",
    "    avg = 'binary' if task == 'binary' else 'weighted'\n",
    "    tuned_acc = accuracy_score(y_test, y_pred)\n",
    "    tuned_f1 = f1_score(y_test, y_pred, average=avg)\n",
    "    print(f\"   Test Accuracy: {tuned_acc:.4f}\")\n",
    "    print(f\"   Test F1-Score: {tuned_f1:.4f}\")\n",
    "\n",
    "    return grid.best_estimator_, grid.best_params_, grid.best_score_, tuned_acc, tuned_f1\n",
    "\n",
    "print(\"\u2705 Hyperparameter tuning function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 \u2014 Tune Binary Classification Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60 + \"\\n\ud83d\udd2c Tuning BREAST CANCER\")\n",
    "X = df_cancer_clean.drop('diagnosis', axis=1); y = df_cancer_clean['diagnosis']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "sc = StandardScaler(); X_tr_s = sc.fit_transform(X_tr); X_te_s = sc.transform(X_te)\n",
    "best_cancer, params_cancer, cv_cancer_t, acc_c, f1_c = tune_best_model(X_tr_s, X_te_s, y_tr, y_te, results_cancer, 'Breast Cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60 + \"\\n\ud83e\ude7a Tuning DIABETES\")\n",
    "X = df_diabetes_clean.drop('Outcome', axis=1); y = df_diabetes_clean['Outcome']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "sc = StandardScaler(); X_tr_s = sc.fit_transform(X_tr); X_te_s = sc.transform(X_te)\n",
    "best_diabetes, params_diab, cv_diab_t, acc_d, f1_d = tune_best_model(X_tr_s, X_te_s, y_tr, y_te, results_diabetes, 'Diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60 + \"\\n\ud83d\udea2 Tuning TITANIC\")\n",
    "X = df_titanic_clean.drop('Survived', axis=1); y = df_titanic_clean['Survived']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "sc = StandardScaler(); X_tr_s = sc.fit_transform(X_tr); X_te_s = sc.transform(X_te)\n",
    "best_titanic, params_tit, cv_tit_t, acc_t, f1_t = tune_best_model(X_tr_s, X_te_s, y_tr, y_te, results_titanic, 'Titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 \u2014 Tune Multi-Class Classification Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60 + \"\\n\ud83c\udf38 Tuning IRIS\")\n",
    "X = df_iris_clean[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n",
    "y = df_iris_clean['Species_encoded']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "sc = StandardScaler(); X_tr_s = sc.fit_transform(X_tr); X_te_s = sc.transform(X_te)\n",
    "best_iris, params_iris_t, cv_iris_t, acc_i, f1_i = tune_best_model(X_tr_s, X_te_s, y_tr, y_te, results_iris, 'Iris', 'multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60 + \"\\n\ud83c\udf77 Tuning WINE QUALITY\")\n",
    "X = df_wine_clean.drop('quality', axis=1); y_raw = df_wine_clean['quality']\n",
    "le_w = LabelEncoder(); y = le_w.fit_transform(y_raw)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "sc = StandardScaler(); X_tr_s = sc.fit_transform(X_tr); X_te_s = sc.transform(X_te)\n",
    "best_wine, params_wine_t, cv_wine_t, acc_w, f1_w = tune_best_model(X_tr_s, X_te_s, y_tr, y_te, results_wine, 'Wine Quality', 'multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60 + \"\\n\ud83e\uded8 Tuning DRY BEAN\")\n",
    "X = df_beans_clean.drop(['Class','Class_encoded'], axis=1); y = df_beans_clean['Class_encoded']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "sc = StandardScaler(); X_tr_s = sc.fit_transform(X_tr); X_te_s = sc.transform(X_te)\n",
    "best_beans, params_beans_t, cv_beans_t, acc_b, f1_b = tune_best_model(X_tr_s, X_te_s, y_tr, y_te, results_beans, 'Dry Bean', 'multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# \ud83d\udcbe Part 10: Save Best Models (joblib)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "best_models_dict = {\n",
    "    'breast_cancer_model': best_cancer,\n",
    "    'diabetes_model': best_diabetes,\n",
    "    'titanic_model': best_titanic,\n",
    "    'iris_model': best_iris,\n",
    "    'wine_quality_model': best_wine,\n",
    "    'dry_bean_model': best_beans\n",
    "}\n",
    "\n",
    "for name, model in best_models_dict.items():\n",
    "    filename = f'{name}.joblib'\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"\ud83d\udcbe Saved: {filename}\")\n",
    "\n",
    "print(\"\\n\u2705 All 6 tuned models saved! You can download them from the Files panel.\")\n",
    "print(\"   To load: model = joblib.load('breast_cancer_model.joblib')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# \ud83d\udcca Part 11: Grand Comparison & Final Summary\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {\n",
    "    'Breast Cancer': results_cancer, 'Diabetes': results_diabetes,\n",
    "    'Titanic': results_titanic, 'Iris': results_iris,\n",
    "    'Wine Quality': results_wine, 'Dry Bean': results_beans\n",
    "}\n",
    "\n",
    "# Grand comparison chart\n",
    "fig, axes = plt.subplots(2, 3, figsize=(24, 12))\n",
    "for idx, (name, res) in enumerate(all_results.items()):\n",
    "    ax = axes[idx//3, idx%3]\n",
    "    x = np.arange(len(res))\n",
    "    w = 0.13\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "    colors = ['#2196F3', '#4CAF50', '#FF9800', '#E91E63', '#9C27B0']\n",
    "    for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "        ax.bar(x + i*w, res[metric], w, label=metric, color=color, alpha=0.85)\n",
    "    ax.set_xticks(x + 2*w)\n",
    "    ax.set_xticklabels(res['Model'], rotation=35, ha='right', fontsize=9)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_title(f'\ud83d\udcca {name}', fontsize=13, fontweight='bold')\n",
    "    if idx == 0: ax.legend(fontsize=8, loc='lower right')\n",
    "plt.suptitle('\ud83c\udfc6 Grand Model Comparison Across All Datasets', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation stability comparison\n",
    "all_cv = {'Breast Cancer': cv_cancer, 'Diabetes': cv_diabetes, 'Titanic': cv_titanic,\n",
    "           'Iris': cv_iris, 'Wine Quality': cv_wine, 'Dry Bean': cv_beans}\n",
    "fig, axes = plt.subplots(2, 3, figsize=(22, 10))\n",
    "for idx, (name, cv_res) in enumerate(all_cv.items()):\n",
    "    ax = axes[idx//3, idx%3]\n",
    "    cv_data = pd.DataFrame(cv_res)\n",
    "    ax.boxplot(cv_data.values, labels=cv_data.columns)\n",
    "    ax.set_title(f'{name}', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('CV F1-Score')\n",
    "    ax.tick_params(axis='x', rotation=35)\n",
    "plt.suptitle('\ud83d\udce6 Cross-Validation Score Distributions', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model per dataset summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83c\udfc6 FINAL SUMMARY \u2014 BEST MODELS PER DATASET\")\n",
    "print(\"=\"*70)\n",
    "summary_data = []\n",
    "for name, res in all_results.items():\n",
    "    best_idx = res['F1-Score'].idxmax()\n",
    "    best = res.iloc[best_idx]\n",
    "    summary_data.append({\n",
    "        'Dataset': name, 'Best Model': best['Model'],\n",
    "        'Accuracy': f\"{best['Accuracy']:.4f}\", 'F1-Score': f\"{best['F1-Score']:.4f}\",\n",
    "        'ROC-AUC': f\"{best['ROC-AUC']:.4f}\", 'CV-Mean': f\"{best['CV-Mean']:.4f}\",\n",
    "        'CV-Std': f\"\u00b1{best['CV-Std']:.4f}\"\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual summary\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "datasets_list = summary_df['Dataset'].tolist()\n",
    "f1_scores = [float(x) for x in summary_df['F1-Score']]\n",
    "acc_scores = [float(x) for x in summary_df['Accuracy']]\n",
    "auc_scores = [float(x) for x in summary_df['ROC-AUC']]\n",
    "cv_scores = [float(x) for x in summary_df['CV-Mean']]\n",
    "\n",
    "x = np.arange(len(datasets_list))\n",
    "w = 0.2\n",
    "ax.bar(x - 1.5*w, acc_scores, w, label='Accuracy', color='#2196F3', alpha=0.85)\n",
    "ax.bar(x - 0.5*w, f1_scores, w, label='F1-Score', color='#E91E63', alpha=0.85)\n",
    "ax.bar(x + 0.5*w, auc_scores, w, label='ROC-AUC', color='#9C27B0', alpha=0.85)\n",
    "ax.bar(x + 1.5*w, cv_scores, w, label='CV-Mean', color='#4CAF50', alpha=0.85)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(datasets_list, rotation=20, ha='right')\n",
    "ax.set_ylim(0, 1.15)\n",
    "ax.set_title('\ud83c\udfc6 Best Model Performance Across All Datasets', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "for i in range(len(datasets_list)):\n",
    "    ax.annotate(summary_df['Best Model'].iloc[i],\n",
    "                (x[i], max(acc_scores[i], f1_scores[i], auc_scores[i], cv_scores[i]) + 0.02),\n",
    "                ha='center', fontsize=8, fontweight='bold', color='green')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 PROJECT COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "\ud83d\udccb Complete ML Pipeline Steps Covered:\n",
    "  \u2705 EDA (Exploratory Data Analysis) \u2014 6 datasets\n",
    "  \u2705 Outlier Detection & Handling (IQR Capping/Winsorization)\n",
    "  \u2705 Data Cleaning & Missing Value Imputation\n",
    "  \u2705 Feature Engineering (Titanic: Title, FamilySize, IsAlone)\n",
    "  \u2705 Feature Selection (ANOVA F-test + Mutual Information)\n",
    "  \u2705 Class Imbalance Handling (SMOTE)\n",
    "  \u2705 Feature Scaling (StandardScaler)\n",
    "  \u2705 6 ML Models \u00d7 6 Datasets = 36 Models Trained\n",
    "  \u2705 5-Fold Stratified Cross-Validation\n",
    "  \u2705 Evaluation: Accuracy, Precision, Recall, F1, ROC-AUC\n",
    "  \u2705 Detailed Classification Reports (per-class)\n",
    "  \u2705 Confusion Matrices & ROC Curves\n",
    "  \u2705 Feature Importance Plots (tree-based models)\n",
    "  \u2705 Learning Curves (overfitting/underfitting check)\n",
    "  \u2705 Model Comparison Charts & Heatmaps\n",
    "  \u2705 Hyperparameter Tuning (GridSearchCV)\n",
    "  \u2705 Model Saving (joblib)\n",
    "  \u2705 Final Summary Report\n",
    "\n",
    "\ud83d\udcca Datasets processed:\n",
    "  Binary:     Breast Cancer | Diabetes | Titanic\n",
    "  Multi-Class: Iris | Wine Quality | Dry Bean\n",
    "\"\"\")"
   ]
  }
 ]
}